"""
FastAPI server for LangChain RAG with Milvus vector store.

This server initializes document loading, splitting, Milvus vector store,
retriever, prompt, and ChatOpenAI once at startup. It exposes an OpenAI-compatible
POST /v1/chat/completions endpoint for question answering and supports parallel requests safely.
"""

import argparse
import asyncio
import os
import time
import uuid
from contextlib import asynccontextmanager
from typing import Optional, List, Literal
from dotenv import load_dotenv, find_dotenv

# Load environment variables at import time so .env is read even when the
# module is imported by an ASGI server (for example, `uvicorn rag_server:app`).
# Prefer the `.env` in the current working directory (where the user invoked
# `uv run`) so project-specific environment settings are picked up. Fall back
# to the default behavior if none is found.
_env_path = find_dotenv(usecwd=True)
if _env_path:
    load_dotenv(_env_path)
else:
    load_dotenv()

import logging

logger = logging.getLogger(__name__)

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from langchain_milvus import Milvus
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.prompts import PromptTemplate
from utils import ai_message_to_chat_completion


# Global variables to store initialized components
qa_chain = None
vectorstore = None


class Message(BaseModel):
    """OpenAI-compatible message model."""
    role: Literal["system", "user", "assistant"] = Field(..., description="The role of the message sender")
    content: str = Field(..., description="The content of the message")


class ChatCompletionRequest(BaseModel):
    """OpenAI-compatible chat completion request model."""
    model: Optional[str] = Field(None, description="ID of the model to use")
    messages: List[Message] = Field(..., description="List of messages in the conversation")
    temperature: Optional[float] = Field(None, description="Sampling temperature between 0 and 2 (not currently used)")
    max_tokens: Optional[int] = Field(None, description="Maximum number of tokens to generate (not currently used)")
    stream: Optional[bool] = Field(False, description="Whether to stream the response (not currently supported)")


class ChatCompletionChoice(BaseModel):
    """OpenAI-compatible choice model."""
    index: int = Field(..., description="The index of the choice")
    message: Message = Field(..., description="The message generated by the model")
    finish_reason: str = Field(..., description="The reason the model stopped generating tokens")


class Usage(BaseModel):
    """OpenAI-compatible usage model."""
    prompt_tokens: int = Field(0, description="Number of tokens in the prompt")
    completion_tokens: int = Field(0, description="Number of tokens in the completion")
    total_tokens: int = Field(0, description="Total number of tokens used")


class ChatCompletionResponse(BaseModel):
    """OpenAI-compatible chat completion response model.
    """
    id: str = Field(..., description="Unique identifier for the chat completion")
    object: str = Field("chat.completion", description="Object type")
    created: int = Field(..., description="Unix timestamp of when the completion was created")
    model: str = Field(..., description="The model used for completion")
    choices: List[ChatCompletionChoice] = Field(..., description="List of completion choices")
    usage: Usage = Field(..., description="Token usage information")


def _extract_assistant_text(obj) -> Optional[str]:
    """Recursively extract the assistant text from various LLM/chain result shapes.

    This handles dicts with keys like 'result', 'text', 'content', or nested
    structures like 'choices' -> message -> content. If the object is a string
    that has appended metadata (e.g., "content='...' additional_kwargs=..."),
    the regex will extract the inner content and strip trailing metadata.
    """
    import re

    # Traverse dict-like structures
    if isinstance(obj, dict):
        # Look for common keys first and prefer direct text fields
        for key in ("result", "text", "content", "answer", "message"):
            if key in obj:
                return _extract_assistant_text(obj[key])

        # choices/message patterns
        if "choices" in obj and isinstance(obj["choices"], (list, tuple)) and obj["choices"]:
            choice = obj["choices"][0]
            if isinstance(choice, dict) and "message" in choice:
                return _extract_assistant_text(choice["message"])
            return _extract_assistant_text(choice)

        # If no direct text found, return a JSON dump of the dict to preserve structure
        # This avoids returning stringified reprs with appended metadata
        try:
            import json
            return json.dumps(obj, ensure_ascii=False)
        except Exception:
            # Fallback: traverse values
            for v in obj.values():
                parsed = _extract_assistant_text(v)
                if parsed:
                    return parsed
            return None

    # Traverse list-like structures
    if isinstance(obj, (list, tuple)):
        for v in obj:
            parsed = _extract_assistant_text(v)
            if parsed:
                return parsed
        return None

    # If it's already a string, try to extract the inner assistant text
    if isinstance(obj, str):
        # Look for content='...'
        m = re.search(r"content=[\'\"](.+?)[\'\"]", obj)
        if m:
            return m.group(1).strip()
        # Strip known appended metadata segments
        s = obj
        for sep in (" additional_kwargs=", " response_metadata=", " usage_metadata=", " id='lc_run", " tool_calls="):
            if sep in s:
                s = s.split(sep)[0].strip()
        # If the string looks like a repr with leading "content=...", fall back to entire string
        return s.strip()

    # Fallback: try to stringify and re-run
    try:
        return _extract_assistant_text(str(obj))
    except Exception:
        return None


def parse_args():
    """Parse command line arguments.

    Uses simple environment-variable defaults (loaded from `.env` at import-time).
    CLI flags override environment variables. Follows same style as `ingest_documents.py`.
    """

    parser = argparse.ArgumentParser(description="FastAPI server for LangChain RAG")
    parser.add_argument(
        "--host",
        type=str,
        default=os.getenv("HOST", os.getenv("APP_HOST", "0.0.0.0")),
        help="Host to bind the server to (default: 0.0.0.0 or from HOST/APP_HOST env var)",
    )
    parser.add_argument(
        "--port",
        type=int,
        default=int(os.getenv("PORT", os.getenv("APP_PORT", "8000"))),
        help="Port to bind the server to (default: 8000 or from PORT/APP_PORT env var)",
    )
    parser.add_argument(
        "--milvus-db",
        type=str,
        default=os.getenv("MILVUS_DB", "./milvus_demo.db"),
        help="Path to local Milvus database file (default: ./milvus_demo.db or MILVUS_DB env var)",
    )
    parser.add_argument(
        "--collection-name",
        type=str,
        default=os.getenv("MILVUS_COLLECTION_NAME", os.getenv("COLLECTION_NAME", "rag_collection")),
        help="Milvus collection name (default: rag_collection or MILVUS_COLLECTION_NAME env var)",
    )
    parser.add_argument(
        "--openai-api-key",
        type=str,
        default=os.getenv("OPENAI_API_KEY", None),
        help="OpenAI API key for LLM (default: reads from OPENAI_API_KEY env var)",
    )
    parser.add_argument(
        "--openai-base-url",
        type=str,
        default=os.getenv("OPENAI_BASE_URL", os.getenv("OPENAI_API_BASE", None)),
        help="OpenAI-compatible base URL for LLM (default: uses OpenAI's default or OPENAI_BASE_URL/OPENAI_API_BASE env var)",
    )
    parser.add_argument(
        "--model-name",
        type=str,
        default=os.getenv("OPENAI_MODEL_NAME", os.getenv("MODEL_NAME", "gpt-3.5-turbo")),
        help="OpenAI model name (default: gpt-3.5-turbo or OPENAI_MODEL_NAME/MODEL_NAME env var)",
    )
    parser.add_argument(
        "--embedding-api-key",
        type=str,
        default=os.getenv("EMBEDDING_API_KEY", os.getenv("OPENAI_EMBEDDING_API_KEY", os.getenv("OPENAI_API_KEY", None))),
        help="API key for embedding model (default: uses same as --openai-api-key or OPENAI_API_KEY env var)",
    )
    parser.add_argument(
        "--embedding-base-url",
        type=str,
        default=os.getenv("EMBEDDING_BASE_URL", None),
        help="OpenAI-compatible base URL for embedding model (default: uses OpenAI's default or EMBEDDING_BASE_URL env var)",
    )
    parser.add_argument(
        "--embedding-model-name",
        type=str,
        default=os.getenv("EMBEDDING_MODEL_NAME", "text-embedding-ada-002"),
        help="Embedding model name (default: text-embedding-ada-002 or EMBEDDING_MODEL_NAME env var)",
    )
    return parser.parse_args()


def initialize_rag_system(args):
    """
    Initialize the RAG system by connecting to existing Milvus vector store,
    setting up retriever, prompt, and ChatOpenAI.
    
    Note: Documents should be pre-loaded using ingest_documents.py
    """
    global qa_chain, vectorstore
    
    print("Initializing RAG system...")

    # Ensure .env from current working directory is loaded at runtime when
    # running via tools like `uv run --with ...` since import-time loading may
    # not always pick up the project .env in every execution environment.
    _env_path_runtime = find_dotenv(usecwd=True)
    if _env_path_runtime:
        load_dotenv(_env_path_runtime, override=False)

    # Debug: reveal whether we picked up .env paths and whether the key is present
    try:
        logger.debug("import-time .env path=%s; runtime .env path=%s; OPENAI_API_KEY in env=%s", _env_path if '_env_path' in globals() else None, _env_path_runtime, ("OPENAI_API_KEY" in os.environ))
    except Exception:
        pass

    # If the key is still not in the environment, try reading it directly from the
    # project's .env file (useful when load_dotenv didn't modify os.environ for any reason).
    if "OPENAI_API_KEY" not in os.environ and not args.openai_api_key:
        from dotenv import dotenv_values
        values = dotenv_values(_env_path_runtime or '')
        if values and values.get("OPENAI_API_KEY"):
            os.environ["OPENAI_API_KEY"] = values.get("OPENAI_API_KEY")

    # Validate OpenAI API key
    if args.openai_api_key:
        os.environ["OPENAI_API_KEY"] = args.openai_api_key

    if "OPENAI_API_KEY" not in os.environ:
        raise ValueError(
            "OpenAI API key must be provided via --openai-api-key or OPENAI_API_KEY env var"
        )
    
    # Initialize embeddings
    print(f"Initializing embeddings (model: {args.embedding_model_name})...")
    embedding_kwargs = {"model": args.embedding_model_name}
    
    # Set embedding API key (defaults to main API key if not specified)
    if args.embedding_api_key:
        embedding_kwargs["openai_api_key"] = args.embedding_api_key or os.environ["OPENAI_API_KEY"]
    
    # Set embedding base URL if provided
    if args.embedding_base_url:
        embedding_kwargs["base_url"] = args.embedding_base_url
    embeddings = OpenAIEmbeddings(**embedding_kwargs)
    
    # Connect to existing Milvus vector store
    print(f"Connecting to Milvus database at {args.milvus_db}...")
    
    # Resolve the Milvus DB path. If a relative path is provided, prefer the
    # current working directory (where the user ran the command). Next, check
    # the module's directory as a fallback (useful when installed into a venv).
    milvus_db_path = args.milvus_db
    attempts = None
    if not os.path.isabs(milvus_db_path):
        attempts = [
            os.path.abspath(os.path.join(os.getcwd(), milvus_db_path)),
            os.path.abspath(os.path.join(os.path.dirname(__file__), milvus_db_path)),
            os.path.abspath(milvus_db_path),
        ]
        found = None
        for p in attempts:
            if os.path.exists(p):
                found = p
                break
        if found:
            milvus_db_path = found
        else:
            # Default to the first attempt in the error message
            milvus_db_path = attempts[0]
    else:
        milvus_db_path = os.path.abspath(milvus_db_path)

    if not os.path.exists(milvus_db_path):
        info = f" (checked: {', '.join(attempts)})" if attempts else ""
        raise ValueError(
            f"Milvus database not found at {args.milvus_db}{info}. "
            f"Please run 'python ingest_documents.py' in the project root to create and populate the database, or set --milvus-db or MILVUS_DB env var to the correct path."
        )

    vectorstore = Milvus(
        embedding_function=embeddings,
        collection_name=args.collection_name,
        connection_args={
            "uri": milvus_db_path,
        },
        index_params={"index_type": "FLAT", "metric_type": "L2"},
    )
    print("✓ Connected to vector store")
    
    # Initialize retriever
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    
    # Create prompt template
    prompt_template = """Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.

Context:
{context}

Question: {question}
Answer:"""
    
    PROMPT = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"],
    )
    
    # Initialize ChatOpenAI
    print(f"Initializing ChatOpenAI with model {args.model_name}...")
    llm_kwargs = {
        "model_name": args.model_name,
        "temperature": 0
    }
    
    # Set custom base URL if provided
    if args.openai_base_url:
        llm_kwargs["base_url"] = args.openai_base_url
    
    llm_instance = ChatOpenAI(**llm_kwargs)
    
    # Create QA chain using LCEL
    qa_chain = (
        RunnableParallel(
            context=retriever,
            question=RunnablePassthrough(),
        )
        | PROMPT
        | llm_instance
    )

    # Expose some initialized components to module-level globals for fallbacks
    global retriever_obj, PROMPT_TEMPLATE, llm
    retriever_obj = retriever
    PROMPT_TEMPLATE = PROMPT
    llm = llm_instance
    
    print("RAG system initialized successfully!")


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifespan context manager for startup and shutdown events."""
    # Startup: Initialize RAG system
    initialize_rag_system(app.state.args)
    yield
    # Shutdown: Clean up resources
    global vectorstore
    if vectorstore:
        print("Cleaning up vector store...")
        # Milvus connections are cleaned up automatically when the process exits


# Create FastAPI app with lifespan
app = FastAPI(
    title="LangChain RAG API",
    description="FastAPI server for question answering using LangChain RAG with Milvus",
    version="1.0.0",
    lifespan=lifespan,
)


@app.get("/")
async def root():
    """Root endpoint returning API information."""
    return {
        "name": "LangChain RAG API",
        "version": "1.0.0",
        "endpoints": {
            "/v1/chat/completions": "POST - Chat completion endpoint (OpenAI-compatible)",
            "/health": "GET - Health check endpoint",
        },
    }


@app.get("/health")
async def health():
    """Health check endpoint."""
    return {"status": "healthy", "rag_initialized": qa_chain is not None}


@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
async def chat_completions(request: ChatCompletionRequest):
    """
    OpenAI-compatible chat completion endpoint.
    
    Supports parallel requests by running qa_chain.invoke() in a thread pool
    to avoid blocking the async event loop.
    
    Note: Currently, streaming, temperature, and max_tokens parameters are not
    supported and will be ignored if provided.
    """
    if qa_chain is None:
        raise HTTPException(status_code=503, detail="RAG system not initialized")
    
    # Check if streaming is requested
    if request.stream:
        raise HTTPException(
            status_code=501,
            detail="Streaming is not currently supported. Please set stream=false or omit the parameter."
        )
    
    # Validate messages
    if not request.messages:
        raise HTTPException(status_code=400, detail="Messages array cannot be empty")
    
    # Extract the last user message as the question
    user_messages = [msg for msg in request.messages if msg.role == "user"]
    if not user_messages:
        raise HTTPException(status_code=400, detail="No user message found in messages array")
    
    question = user_messages[-1].content.strip()
    if not question:
        raise HTTPException(status_code=400, detail="User message content cannot be empty")
    
    # Prepare a place to store any raw response (from the chain or a forwarded OpenAI-compatible endpoint)
    raw_response = None

    try:
        # Run the synchronous qa_chain.invoke() in a thread pool and return OpenAI-compatible dict directly.
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,
            qa_chain.invoke,
            question
        )
        result = ai_message_to_chat_completion(result)  # openai format dict.
        # Assume dict return is guaranteed; return it directly and let FastAPI validate.
        return result

    except Exception as e:
        # If the pipeline fails (e.g., type mismatch between prompt and llm), prefer
        # forwarding the original request to a configured OpenAI-compatible base URL
        # (for example, a local vLLM instance). If forwarding is not configured or
        # fails, do NOT perform the retriever/document fallback — instead echo the
        # user's question as the assistant response (keeps behavior simple and
        # predictable for clients that expect model-only behavior).

        forwarded_success = False
        forward_error = None
        try:
            base_url = None
            try:
                base_url = getattr(app.state.args, "openai_base_url", None)
            except Exception:
                base_url = None

            if base_url:
                import requests
                forward_url = base_url.rstrip("/") + "/v1/chat/completions"
                try:
                    resp = requests.post(
                        forward_url,
                        json={"model": request.model, "messages": [m.dict() for m in request.messages]},
                        timeout=10,
                    )
                    if resp.status_code == 200:
                        forwarded = resp.json()
                        # If forwarded response looks like an OpenAI-compatible response,
                        # map it into our ChatCompletionChoice/Usage types.
                        if isinstance(forwarded, dict) and "choices" in forwarded:
                            choices = []
                            for i, ch in enumerate(forwarded["choices"]):
                                role = "assistant"
                                content = None
                                if isinstance(ch, dict) and "message" in ch and isinstance(ch["message"], dict):
                                    role = ch["message"].get("role", role)
                                    content = _extract_assistant_text(ch["message"]) or _extract_assistant_text(ch["message"].get("content"))
                                else:
                                    content = _extract_assistant_text(ch) or str(ch)

                                content = content or "No answer generated"
                                idx = ch.get("index", i) if isinstance(ch, dict) else i
                                finish_reason = ch.get("finish_reason", "stop") if isinstance(ch, dict) else "stop"
                                choices.append(ChatCompletionChoice(index=idx, message=Message(role=role, content=content), finish_reason=finish_reason))

                            usage = _extract_usage(forwarded)
                            # Preserve the raw forwarded response for debugging / client use
                            raw_response = forwarded
                            forwarded_success = True
                except Exception as fe:
                    forward_error = fe
        except Exception as outer_fe:
            forward_error = outer_fe

        if not forwarded_success:
            # Log the original pipeline error and any forwarding error without full traceback
            # (avoid noisy stack traces for expected pipeline errors; enable DEBUG to see full details)
            logger.error(f"qa_chain.invoke failed: {e}")
            if forward_error is not None:
                logger.error(f"Forwarding to external base URL failed: {forward_error}")
            # If DEBUG enabled, log full traceback for diagnostics
            logger.debug("Full exception details for qa_chain.invoke/forwarding", exc_info=True)

            # No retriever-based fallback per user request — echo the user's question
            choices = [
                ChatCompletionChoice(
                    index=0,
                    message=Message(role="assistant", content=question),
                    finish_reason="stop",
                )
            ]
            usage = Usage(prompt_tokens=0, completion_tokens=0, total_tokens=0)

    # Create OpenAI-compatible response
    completion_id = f"chatcmpl-{uuid.uuid4().hex[:24]}"
    created_timestamp = int(time.time())
    
    response = ChatCompletionResponse(
        id=completion_id,
        object="chat.completion",
        created=created_timestamp,
        model=request.model or "gpt-3.5-turbo",
        choices=choices,
        usage=usage,
    )
    
    return response


def main():
    """Main entry point for the application."""
    import uvicorn
    
    # Load environment variables from the project's .env (prefer CWD),
    # ensuring CLI defaults (parse_args) see project .env when running via `uv run`.
    _env_path_main = find_dotenv(usecwd=True)
    if _env_path_main:
        # Force-load project .env so that running via `uv run --with ...` picks up
        # project environment values (override existing env vars if necessary).
        print(f"LOADING .env from project path: {_env_path_main}")
        load_dotenv(_env_path_main, override=True)
    else:
        print("No project .env found via find_dotenv(); falling back to default load_dotenv()")
        load_dotenv()

    # Parse command line arguments
    args = parse_args()
    logger.debug("Parsed CLI args; openai_api_key provided=%s; OPENAI_API_KEY in env=%s", bool(args.openai_api_key), ("OPENAI_API_KEY" in os.environ))
    
    # Store args in app state so they're accessible in lifespan
    app.state.args = args
    
    # Run the server with uvicorn
    print(f"Starting server on {args.host}:{args.port}")
    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
    )


if __name__ == "__main__":
    main()
