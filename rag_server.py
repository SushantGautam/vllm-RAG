"""
FastAPI server for LangChain RAG with Milvus vector store.

This server initializes document loading, splitting, Milvus vector store,
retriever, prompt, and ChatOpenAI once at startup. It exposes an OpenAI-compatible
POST /v1/chat/completions endpoint for question answering and supports parallel requests safely.
"""

import argparse
import asyncio
import os
import time
import uuid
from contextlib import asynccontextmanager
from typing import Optional, List, Literal

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from langchain_community.vectorstores import Milvus
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate


# Global variables to store initialized components
qa_chain = None
vectorstore = None


class Message(BaseModel):
    """OpenAI-compatible message model."""
    role: Literal["system", "user", "assistant"] = Field(..., description="The role of the message sender")
    content: str = Field(..., description="The content of the message")


class ChatCompletionRequest(BaseModel):
    """OpenAI-compatible chat completion request model."""
    model: Optional[str] = Field(None, description="ID of the model to use")
    messages: List[Message] = Field(..., description="List of messages in the conversation")
    temperature: Optional[float] = Field(None, description="Sampling temperature between 0 and 2 (not currently used)")
    max_tokens: Optional[int] = Field(None, description="Maximum number of tokens to generate (not currently used)")
    stream: Optional[bool] = Field(False, description="Whether to stream the response (not currently supported)")


class ChatCompletionChoice(BaseModel):
    """OpenAI-compatible choice model."""
    index: int = Field(..., description="The index of the choice")
    message: Message = Field(..., description="The message generated by the model")
    finish_reason: str = Field(..., description="The reason the model stopped generating tokens")


class Usage(BaseModel):
    """OpenAI-compatible usage model."""
    prompt_tokens: int = Field(0, description="Number of tokens in the prompt")
    completion_tokens: int = Field(0, description="Number of tokens in the completion")
    total_tokens: int = Field(0, description="Total number of tokens used")


class ChatCompletionResponse(BaseModel):
    """OpenAI-compatible chat completion response model."""
    id: str = Field(..., description="Unique identifier for the chat completion")
    object: str = Field("chat.completion", description="Object type")
    created: int = Field(..., description="Unix timestamp of when the completion was created")
    model: str = Field(..., description="The model used for completion")
    choices: List[ChatCompletionChoice] = Field(..., description="List of completion choices")
    usage: Usage = Field(..., description="Token usage information")


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="FastAPI server for LangChain RAG")
    parser.add_argument(
        "--host",
        type=str,
        default="0.0.0.0",
        help="Host to bind the server to (default: 0.0.0.0)",
    )
    parser.add_argument(
        "--port",
        type=int,
        default=8000,
        help="Port to bind the server to (default: 8000)",
    )
    parser.add_argument(
        "--milvus-db",
        type=str,
        default="./milvus_demo.db",
        help="Path to local Milvus database file (default: ./milvus_demo.db)",
    )
    parser.add_argument(
        "--collection-name",
        type=str,
        default="rag_collection",
        help="Milvus collection name (default: rag_collection)",
    )
    parser.add_argument(
        "--openai-api-key",
        type=str,
        default=None,
        help="OpenAI API key for LLM (default: reads from OPENAI_API_KEY env var)",
    )
    parser.add_argument(
        "--openai-base-url",
        type=str,
        default=None,
        help="OpenAI-compatible base URL for LLM (default: uses OpenAI's default)",
    )
    parser.add_argument(
        "--model-name",
        type=str,
        default="gpt-3.5-turbo",
        help="OpenAI model name (default: gpt-3.5-turbo)",
    )
    parser.add_argument(
        "--embedding-api-key",
        type=str,
        default=None,
        help="API key for embedding model (default: uses same as --openai-api-key or OPENAI_API_KEY env var)",
    )
    parser.add_argument(
        "--embedding-base-url",
        type=str,
        default=None,
        help="OpenAI-compatible base URL for embedding model (default: uses OpenAI's default)",
    )
    parser.add_argument(
        "--embedding-model-name",
        type=str,
        default="text-embedding-ada-002",
        help="Embedding model name (default: text-embedding-ada-002)",
    )
    return parser.parse_args()


def initialize_rag_system(args):
    """
    Initialize the RAG system by connecting to existing Milvus vector store,
    setting up retriever, prompt, and ChatOpenAI.
    
    Note: Documents should be pre-loaded using ingest_documents.py
    """
    global qa_chain, vectorstore
    
    print("Initializing RAG system...")
    
    # Validate OpenAI API key
    if args.openai_api_key:
        os.environ["OPENAI_API_KEY"] = args.openai_api_key
    
    if "OPENAI_API_KEY" not in os.environ:
        raise ValueError(
            "OpenAI API key must be provided via --openai-api-key or OPENAI_API_KEY env var"
        )
    
    # Initialize embeddings
    print(f"Initializing embeddings (model: {args.embedding_model_name})...")
    embedding_kwargs = {"model": args.embedding_model_name}
    
    # Set embedding API key (defaults to main API key if not specified)
    if args.embedding_api_key:
        embedding_kwargs["openai_api_key"] = args.embedding_api_key
    
    # Set embedding base URL if provided
    if args.embedding_base_url:
        embedding_kwargs["base_url"] = args.embedding_base_url
    
    embeddings = OpenAIEmbeddings(**embedding_kwargs)
    
    # Connect to existing Milvus vector store
    print(f"Connecting to Milvus database at {args.milvus_db}...")
    
    if not os.path.exists(args.milvus_db):
        raise ValueError(
            f"Milvus database not found at {args.milvus_db}. "
            f"Please run 'python ingest_documents.py' first to create and populate the database."
        )
    
    vectorstore = Milvus(
        embedding_function=embeddings,
        collection_name=args.collection_name,
        connection_args={"uri": args.milvus_db},
    )
    print("âœ“ Connected to vector store")
    
    # Initialize retriever
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    
    # Create prompt template
    prompt_template = """Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.

Context:
{context}

Question: {question}
Answer:"""
    
    PROMPT = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"],
    )
    
    # Initialize ChatOpenAI
    print(f"Initializing ChatOpenAI with model {args.model_name}...")
    llm_kwargs = {
        "model_name": args.model_name,
        "temperature": 0
    }
    
    # Set custom base URL if provided
    if args.openai_base_url:
        llm_kwargs["base_url"] = args.openai_base_url
    
    llm = ChatOpenAI(**llm_kwargs)
    
    # Create QA chain
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": PROMPT},
        return_source_documents=False,
    )
    
    print("RAG system initialized successfully!")


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifespan context manager for startup and shutdown events."""
    # Startup: Initialize RAG system
    initialize_rag_system(app.state.args)
    yield
    # Shutdown: Clean up resources
    global vectorstore
    if vectorstore:
        print("Cleaning up vector store...")
        # Milvus connections are cleaned up automatically when the process exits


# Create FastAPI app with lifespan
app = FastAPI(
    title="LangChain RAG API",
    description="FastAPI server for question answering using LangChain RAG with Milvus",
    version="1.0.0",
    lifespan=lifespan,
)


@app.get("/")
async def root():
    """Root endpoint returning API information."""
    return {
        "name": "LangChain RAG API",
        "version": "1.0.0",
        "endpoints": {
            "/v1/chat/completions": "POST - Chat completion endpoint (OpenAI-compatible)",
            "/health": "GET - Health check endpoint",
        },
    }


@app.get("/health")
async def health():
    """Health check endpoint."""
    return {"status": "healthy", "rag_initialized": qa_chain is not None}


@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
async def chat_completions(request: ChatCompletionRequest):
    """
    OpenAI-compatible chat completion endpoint.
    
    Supports parallel requests by running qa_chain.invoke() in a thread pool
    to avoid blocking the async event loop.
    
    Note: Currently, streaming, temperature, and max_tokens parameters are not
    supported and will be ignored if provided.
    """
    if qa_chain is None:
        raise HTTPException(status_code=503, detail="RAG system not initialized")
    
    # Check if streaming is requested
    if request.stream:
        raise HTTPException(
            status_code=501,
            detail="Streaming is not currently supported. Please set stream=false or omit the parameter."
        )
    
    # Validate messages
    if not request.messages:
        raise HTTPException(status_code=400, detail="Messages array cannot be empty")
    
    # Extract the last user message as the question
    user_messages = [msg for msg in request.messages if msg.role == "user"]
    if not user_messages:
        raise HTTPException(status_code=400, detail="No user message found in messages array")
    
    question = user_messages[-1].content.strip()
    if not question:
        raise HTTPException(status_code=400, detail="User message content cannot be empty")
    
    try:
        # Run the synchronous qa_chain.invoke() in a thread pool
        # to support parallel requests without blocking
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,  # Use default executor (ThreadPoolExecutor)
            qa_chain.invoke,
            {"query": question}
        )
        
        answer = result.get("result", "No answer generated")
        
        # Create OpenAI-compatible response
        completion_id = f"chatcmpl-{uuid.uuid4().hex[:24]}"
        created_timestamp = int(time.time())
        
        response = ChatCompletionResponse(
            id=completion_id,
            object="chat.completion",
            created=created_timestamp,
            model=request.model or "gpt-3.5-turbo",
            choices=[
                ChatCompletionChoice(
                    index=0,
                    message=Message(role="assistant", content=answer),
                    finish_reason="stop"
                )
            ],
            usage=Usage(
                prompt_tokens=0,  # Not calculated in this implementation
                completion_tokens=0,  # Not calculated in this implementation
                total_tokens=0  # Not calculated in this implementation
            )
        )
        
        return response
    
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error processing chat completion: {str(e)}"
        )


def main():
    """Main entry point for the application."""
    import uvicorn
    
    # Parse command line arguments
    args = parse_args()
    
    # Store args in app state so they're accessible in lifespan
    app.state.args = args
    
    # Run the server with uvicorn
    print(f"Starting server on {args.host}:{args.port}")
    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
    )


if __name__ == "__main__":
    main()
